{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b4d493f-8c8d-4b9a-80b7-746a0fbee68d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# src 모듈 import\n",
    "sys.path.append(\"/Workspace/Users/1dt003@msacademy.msai.kr/team4-CICD/src\")\n",
    "from src.config_loader import load_config\n",
    "from src.data_preprocessor import preprocess_dataframe\n",
    "from src.model_serving import predict_with_model, predict_with_fallback\n",
    "\n",
    "# ==============================\n",
    "# Config 로드\n",
    "# ==============================\n",
    "config = load_config(\"/Workspace/Users/1dt003@msacademy.msai.kr/team4-CICD/configs/config_v2.json\")\n",
    "\n",
    "# 환경변수 우선 적용\n",
    "BASE_MODEL_NAME = os.environ.get(\"BASE_MODEL_NAME\") or config.get(\"model_name\", \"team4-pred-model\")\n",
    "JDBC_URL = os.environ.get(\"JDBC_URL\", config['jdbc_url'])\n",
    "BATCH_TABLE = os.environ.get(\"BATCH_TABLE\", config['batch_table'])\n",
    "TARGET = config.get(\"target\", \"realScore_clean\")\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# ==============================\n",
    "# MLflow 최신 모델 로드\n",
    "# ==============================\n",
    "latest_versions = client.get_latest_versions(name=BASE_MODEL_NAME, stages=[\"None\"])\n",
    "if not latest_versions:\n",
    "    raise ValueError(f\"No versions found for {BASE_MODEL_NAME}\")\n",
    "\n",
    "latest_version_num = max([int(v.version) for v in latest_versions])\n",
    "model_uri = f\"models:/{BASE_MODEL_NAME}/{latest_version_num}\"\n",
    "model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "# ==============================\n",
    "# 샘플 데이터 로드\n",
    "# ==============================\n",
    "connection_properties = config['connection_properties']\n",
    "df_batch = spark.read.jdbc(url=JDBC_URL, table=BATCH_TABLE, properties=connection_properties)\n",
    "\n",
    "# ==============================\n",
    "# 컬럼 전처리\n",
    "# ==============================\n",
    "required_cols = [\"difficultyLevel\", \"guessLevel\", \"discriminationLevel\"]\n",
    "for col in required_cols:\n",
    "    if col not in df_batch.columns and f\"{col}_mean\" in df_batch.columns:\n",
    "        df_batch = df_batch.withColumnRenamed(f\"{col}_mean\", col)\n",
    "\n",
    "if \"correctRate\" not in df_batch.columns:\n",
    "    df_batch = df_batch.withColumn(\"correctRate\", (F.col(\"correct_cnt\") / F.col(\"items_attempted\")).cast(\"double\"))\n",
    "df_batch = df_batch.withColumn(\"accuracy\", F.col(\"correctRate\"))\n",
    "\n",
    "# 필수 컬럼 기본값 처리\n",
    "default_values = {\"age\": 0, \"subjectName\": \"unknown\", \"itemType\": \"unknown\",\n",
    "                  \"answerTime_avg\": 0.0, \"theta_clean\": 0.0}\n",
    "for col_name, default_val in default_values.items():\n",
    "    if col_name not in df_batch.columns:\n",
    "        df_batch = df_batch.withColumn(col_name, F.lit(default_val))\n",
    "\n",
    "df_pandas = df_batch.toPandas()\n",
    "\n",
    "try:\n",
    "    df_processed, categorical_cols = preprocess_dataframe(\n",
    "        df_pandas,\n",
    "        categorical_candidates=config.get(\"categorical_candidates\", [\"gender\", \"grade\"]),\n",
    "        version=\"v2\"\n",
    "    )\n",
    "except:\n",
    "    df_processed = df_pandas\n",
    "    categorical_cols = config.get(\"categorical_candidates\", [\"gender\", \"grade\", \"subjectName\", \"itemType\"])\n",
    "\n",
    "# ==============================\n",
    "# 모델 예측 + RMSE 계산\n",
    "# ==============================\n",
    "deploy_decision = False\n",
    "rmse_estimate = None\n",
    "try:\n",
    "    feature_cols = [c for c in df_processed.columns if c not in config.get(\"exclude_cols\", []) + [TARGET]]\n",
    "\n",
    "    preds = predict_with_fallback(\n",
    "        primary_model=model,\n",
    "        fallback_model=model,\n",
    "        df=df_processed,\n",
    "        feature_cols=feature_cols\n",
    "    )\n",
    "\n",
    "    rmse_estimate = ((preds - df_processed[TARGET]) ** 2).mean() ** 0.5\n",
    "    threshold_rmse = float(os.environ.get(\"STAGING_RMSE_THRESHOLD\", config.get(\"staging_rmse_threshold\", 1e12)))\n",
    "\n",
    "    deploy_decision = rmse_estimate <= threshold_rmse\n",
    "    print(f\"Staging RMSE: {rmse_estimate:.4f}, deploy_decision: {deploy_decision}\")\n",
    "except Exception as e:\n",
    "    print(f\"Prediction failed: {e}\")\n",
    "    deploy_decision = False\n",
    "\n",
    "# ==============================\n",
    "# Notebook Exit → CI/CD workflow 전달\n",
    "# ==============================\n",
    "dbutils.notebook.exit(json.dumps({\n",
    "    \"deploy\": bool(deploy_decision),\n",
    "    \"model_version\": int(latest_version_num),\n",
    "    \"rmse_estimate\": float(rmse_estimate) if rmse_estimate is not None else None\n",
    "}))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "staging_model_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
