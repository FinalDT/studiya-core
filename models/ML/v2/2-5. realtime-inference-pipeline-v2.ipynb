{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e33849fb-28e9-42e8-b8e6-b31185e9bc06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "from math import sqrt\n",
    "import mlflow.pyfunc\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "from io import BytesIO\n",
    "from azure.storage.blob import BlobServiceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "412a713a-6357-4381-b24d-828d8e38178c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 실시간 추론 모델 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "969b9357-62ff-4193-9734-f9c41193dce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "class RealtimeRealScoreModel(mlflow.pyfunc.PythonModel):\n",
    "\n",
    "    def load_context(self, context):\n",
    "        import mlflow.sklearn\n",
    "        self.model = mlflow.sklearn.load_model(context.artifacts[\"model\"])\n",
    "\n",
    "    def fetch_bronze_question(self):\n",
    "        blob_connection_string = \"DefaultEndpointsProtocol=https;AccountName=team4mlblob;AccountKey=qU3qjqdPjn/LlGZzIfI/ox6zVb6BhIo1Dn1PRr4akHTJLlpQ9x8rHbtZsRFvTCgjh5Qpn/4td+q3+AStbBi+PQ==;EndpointSuffix=core.windows.net\"\n",
    "        container_name = \"blobml\"\n",
    "        blob_name = \"bronze_questions.csv\"\n",
    "\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(blob_connection_string)\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "        download_stream = blob_client.download_blob()\n",
    "        df_bronze = pd.read_csv(BytesIO(download_stream.readall()))\n",
    "        \n",
    "        return df_bronze\n",
    "\n",
    "    def prepare_input(self, df_input):\n",
    "        \"\"\"입력 df와 bronze_question 조인 (input 우선, _x/_y 제거)\"\"\"\n",
    "        df_bronze = self.fetch_bronze_question()\n",
    "\n",
    "        if not df_input.empty and not df_bronze.empty:\n",
    "            join_cols = [\"testID\", \"assessmentItemID\"]\n",
    "            for col in join_cols:\n",
    "                if col in df_input.columns and col in df_bronze.columns:\n",
    "                    df_input[col] = df_input[col].astype(str)\n",
    "                    df_bronze[col] = df_bronze[col].astype(str)\n",
    "\n",
    "            # merge (기본 _x, _y suffix 사용)\n",
    "            df_merged = pd.merge(df_input, df_bronze, on=join_cols, how=\"left\")\n",
    "\n",
    "            # 입력값 우선으로 모든 겹치는 컬럼 처리\n",
    "            # 먼저 _x 컬럼들을 처리\n",
    "            for col in list(df_merged.columns):\n",
    "                if col.endswith('_x'):\n",
    "                    base_col = col[:-2]  # '_x' 제거\n",
    "                    bronze_col = f\"{base_col}_y\"\n",
    "                    \n",
    "                    if bronze_col in df_merged.columns:\n",
    "                        # 입력값 우선으로 결합 (입력값이 없을 때만 bronze 값 사용)\n",
    "                        df_merged[base_col] = df_merged[col].combine_first(df_merged[bronze_col])\n",
    "                    else:\n",
    "                        # bronze에 없는 컬럼은 그냥 원래 이름으로 변경\n",
    "                        df_merged[base_col] = df_merged[col]\n",
    "            \n",
    "            # 이제 _x, _y suffix 컬럼들을 모두 삭제\n",
    "            suffix_cols = [col for col in df_merged.columns if col.endswith('_x') or col.endswith('_y')]\n",
    "            df_merged = df_merged.drop(columns=suffix_cols)\n",
    "\n",
    "        else:\n",
    "            df_merged = df_bronze.copy()\n",
    "\n",
    "        # 결측치 처리 및 숫자 변환\n",
    "        required_cols = ['is_correct', 'grade', 'gender', 'discriminationLevel', 'difficultyLevel', 'guessLevel']\n",
    "        df_merged = df_merged.reindex(columns=df_merged.columns.union(required_cols))\n",
    "\n",
    "        for col in required_cols:\n",
    "            df_merged[col] = pd.to_numeric(df_merged[col], errors='coerce')\n",
    "\n",
    "        return df_merged\n",
    "    \n",
    "    # theta 추정\n",
    "    def estimate_theta(self, a_list, b_list, c_list, y_list, grid_n=161):\n",
    "        # (원래 코드 그대로)\n",
    "        import numpy as np\n",
    "        from scipy.optimize import minimize\n",
    "        from scipy.stats import norm\n",
    "        from math import sqrt\n",
    "\n",
    "        def p_3pl(theta, a, b, c):\n",
    "            z = np.outer(theta, a) - a * b\n",
    "            sigmoid = 1 / (1 + np.exp(-z))\n",
    "            return c + (1 - c) * sigmoid\n",
    "\n",
    "        def neg_loglike(th):\n",
    "            probs = p_3pl(np.array([th[0]]), a_list, b_list, c_list)[0]\n",
    "            probs = np.clip(probs, 1e-12, 1-1e-12)\n",
    "            ll = y_list * np.log(probs) + (1-y_list) * np.log(1-probs)\n",
    "            return -np.sum(ll)\n",
    "\n",
    "        try:\n",
    "            res = minimize(neg_loglike, x0=[0.0], bounds=[(-4.0, 4.0)])\n",
    "            theta_mle = float(res.x[0]) if res.success else np.nan\n",
    "        except:\n",
    "            theta_mle = np.nan\n",
    "\n",
    "        theta_grid = np.linspace(-4, 4, grid_n)\n",
    "        probs_grid = p_3pl(theta_grid, a_list, b_list, c_list)\n",
    "        probs_grid = np.clip(probs_grid, 1e-12, 1-1e-12)\n",
    "        log_lik = np.sum(y_list*np.log(probs_grid) + (1-y_list)*np.log(1-probs_grid), axis=1)\n",
    "        log_prior = norm.logpdf(theta_grid, 0, 1)\n",
    "        log_post = log_lik + log_prior\n",
    "        post = np.exp(log_post - np.max(log_post))\n",
    "        post /= post.sum()\n",
    "\n",
    "        theta_eap = float((post * theta_grid).sum())\n",
    "        theta_sd = sqrt(((theta_grid - theta_eap)**2 * post).sum())\n",
    "        expected_score = float(p_3pl([theta_eap], a_list, b_list, c_list)[0].sum())\n",
    "        return theta_mle, theta_eap, theta_sd, expected_score\n",
    "\n",
    "    def calculate_theta_features(self, df_input):\n",
    "        results = []\n",
    "        if len(df_input) > 0:\n",
    "            for (learner, test), g in df_input.groupby(['learnerID', 'testID']):\n",
    "                a_list = g['discriminationLevel'].values\n",
    "                b_list = g['difficultyLevel'].values\n",
    "                c_list = g['guessLevel'].values\n",
    "                y_list = g['is_correct'].astype(int).values\n",
    "                \n",
    "                theta_mle, theta_eap, theta_sd, expected_score = self.estimate_theta(a_list, b_list, c_list, y_list)\n",
    "                \n",
    "                results.append({\n",
    "                    'learnerID': learner,\n",
    "                    'gender': g['gender'].iloc[0],\n",
    "                    'grade': g['grade'].iloc[0],\n",
    "                    'testID': test,\n",
    "                    'theta_clean': theta_eap,\n",
    "                    'theta_sd': theta_sd,\n",
    "                    'difficultyLevel': g['difficultyLevel'].mean(),\n",
    "                    'discriminationLevel': g['discriminationLevel'].mean(),\n",
    "                    'guessLevel': g['guessLevel'].mean(),\n",
    "                    'correct_cnt': int(y_list.sum()),\n",
    "                    'items_attempted': len(y_list),\n",
    "                    'expected_score': expected_score\n",
    "                })\n",
    "        else:\n",
    "            results.append({k: None for k in [\n",
    "                'learnerID', 'gender', 'grade', 'testID', 'theta_clean', 'theta_sd',\n",
    "                'difficultyLevel', 'discriminationLevel', 'guessLevel',\n",
    "                'correct_cnt', 'items_attempted', 'expected_score'\n",
    "            ]})\n",
    "        \n",
    "        df_features = pd.DataFrame(results)\n",
    "        df_features['accuracy'] = df_features['correct_cnt'].fillna(0) / df_features['items_attempted'].replace(0, 1)\n",
    "        return df_features\n",
    "\n",
    "    def predict(self, context, model_input: pd.DataFrame):\n",
    "        try:\n",
    "            # 1️⃣ SQL에서 bronze_question 조인\n",
    "            df_merged = self.prepare_input(model_input)\n",
    "\n",
    "            # 2️⃣ Theta 계산\n",
    "            df_features = self.calculate_theta_features(df_merged)\n",
    "\n",
    "            # 3️⃣ 예측\n",
    "            feature_columns = [\n",
    "                'theta_clean', 'accuracy', 'grade', 'gender',\n",
    "                'difficultyLevel', 'discriminationLevel', 'guessLevel'\n",
    "            ]\n",
    "            for col in feature_columns:\n",
    "                df_features[col] = pd.to_numeric(df_features.get(col, 0), errors='coerce').fillna(0)\n",
    "\n",
    "            if not df_features.empty and all(col in df_features.columns for col in feature_columns):\n",
    "                features_for_prediction = df_features[feature_columns]\n",
    "                df_features['realScore_clean'] = self.model.predict(features_for_prediction)\n",
    "            else:\n",
    "                df_features['realScore_clean'] = 0\n",
    "\n",
    "            return df_features\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"예측 중 오류 발생: {str(e)}\")\n",
    "            return pd.DataFrame({'error': [str(e)]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bb1ecbb-dada-4eff-9a5c-d1d1c0e085cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 실시간 추론 서비스 배포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0488eb3-8b1a-4272-93ef-42ad0f2248d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MLflow 설정\n",
    "EXPERIMENT_NAME = \"/Users/1dt003@msacademy.msai.kr/team4_inference_experiment\"\n",
    "REALTIME_MODEL_NAME = \"realtimeinference-model\"\n",
    "BASE_MODEL_NAME = \"real-score-model\"  # 기존에 저장된 모델명\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "client = MlflowClient()\n",
    "\n",
    "print(\"=== 실시간 추론 모델 배포 시작 ===\")\n",
    "\n",
    "# 기존 저장된 모델의 최신 버전 정보 가져오기\n",
    "try:\n",
    "    base_model_version = client.get_latest_versions(BASE_MODEL_NAME, stages=[\"Stag\"])[0]\n",
    "    base_model_uri = f\"models:/{BASE_MODEL_NAME}/{base_model_version.version}\"\n",
    "    print(f\"기존 모델 로드: {base_model_uri}\")\n",
    "except Exception as e:\n",
    "    print(f\"기존 모델 로드 실패: {e}\")\n",
    "\n",
    "# 실시간 추론 모델 배포\n",
    "with mlflow.start_run(run_name=\"realtime_inference_v2_deployment\") as run:\n",
    "    \n",
    "    # 실시간 추론 모델을 MLflow에 등록\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"realtime_model\",\n",
    "        python_model=RealtimeRealScoreModel(),\n",
    "        artifacts={\"model\": base_model_uri},  # 기존 모델을 artifact로 포함\n",
    "        registered_model_name=REALTIME_MODEL_NAME,\n",
    "        pip_requirements=[\n",
    "            \"mlflow\",\n",
    "            \"pandas\",\n",
    "            \"numpy\",\n",
    "            \"scipy\",\n",
    "            \"scikit-learn\",\n",
    "            \"azure-storage-blob\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    realtime_run_id = run.info.run_id\n",
    "    print(f\"실시간 모델 배포 완료. Run ID: {realtime_run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7b7466-212f-455d-a119-d0bab1249187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 모델 서빙 준비 & 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45f98ad2-2f4c-4267-aea2-605d9856628d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 배포된 모델 로드\n",
    "model_uri = f\"models:/{REALTIME_MODEL_NAME}/latest\"\n",
    "print(f\"모델 로드 중: {model_uri}\")\n",
    "\n",
    "try:\n",
    "    loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "    print(\"모델 로드 성공!\")\n",
    "except Exception as e:\n",
    "    print(f\"모델 로드 실패: {e}\")\n",
    "    # 대안: run_id 직접 사용\n",
    "    model_uri = f\"runs:/{realtime_run_id}/realtime_model\"\n",
    "    loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "    print(\"Run ID로 모델 로드 성공!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43793df3-0b05-43a5-bbe1-236234061d07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 테스트 데이터 준비 및 추론 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad52a7ce-3b9f-49a2-80aa-ad774d8439cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n=== 실시간 추론 테스트 시작 ===\")\n",
    "\n",
    "# 테스트 데이터 1: 기본 케이스\n",
    "test_input_1 = pd.DataFrame({\n",
    "    'learnerID': [1, 1, 1, 2, 2, 2],\n",
    "    'testID': [100, 100, 100, 100, 100, 100],\n",
    "    'assessmentItemID': [101, 102, 103, 101, 102, 103],\n",
    "    'discriminationLevel': [1.2, 0.8, 1.5, 1.2, 0.8, 1.5],\n",
    "    'difficultyLevel': [0.5, -0.2, 1.1, 0.5, -0.2, 1.1],\n",
    "    'guessLevel': [0.1, 0.15, 0.2, 0.1, 0.15, 0.2],\n",
    "    'is_correct': [1, 0, 1, 0, 1, 0],\n",
    "    'grade': [9, 9, 9, 10, 10, 10],\n",
    "    'gender': ['M', 'M', 'M', 'F', 'F', 'F']\n",
    "})\n",
    "\n",
    "# 테스트 데이터 2: 다른 학습자\n",
    "test_input_2 = pd.DataFrame({\n",
    "    'learnerID': [3, 3, 3, 4, 4],\n",
    "    'testID': [101, 101, 101, 101, 101],\n",
    "    'assessmentItemID': [201, 202, 203, 201, 202],\n",
    "    'discriminationLevel': [1.0, 1.3, 0.9, 1.0, 1.3],\n",
    "    'difficultyLevel': [0.2, 0.8, -0.1, 0.2, 0.8],\n",
    "    'guessLevel': [0.12, 0.18, 0.25, 0.12, 0.18],\n",
    "    'is_correct': [1, 1, 0, 1, 0],\n",
    "    'grade': [11, 11, 11, 12, 12],\n",
    "    'gender': ['M', 'M', 'M', 'F', 'F']\n",
    "})\n",
    "\n",
    "# 테스트 실행\n",
    "test_cases = [\n",
    "    (\"기본 테스트 케이스\", test_input_1),\n",
    "    (\"다른 학습자 테스트\", test_input_2)\n",
    "]\n",
    "\n",
    "for test_name, test_data in test_cases:\n",
    "    print(f\"\\n--- {test_name} ---\")\n",
    "    print(\"입력 데이터:\")\n",
    "    print(test_data.head())\n",
    "    \n",
    "    try:\n",
    "        # 추론 실행\n",
    "        prediction = loaded_model.predict(test_data)\n",
    "        \n",
    "        print(\"예측 결과:\")\n",
    "        if 'error' in prediction.columns:\n",
    "            print(f\"오류 발생: {prediction['error'].iloc[0]}\")\n",
    "        else:\n",
    "            result_columns = ['learnerID', 'testID', 'theta_clean', 'accuracy', 'realScore_clean']\n",
    "            available_columns = [col for col in result_columns if col in prediction.columns]\n",
    "            print(prediction[available_columns])\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"추론 실행 중 오류: {str(e)}\")\n",
    "\n",
    "\n",
    "# 성능 테스트 (응답 시간)\n",
    "print(\"\\n=== 성능 테스트 ===\")\n",
    "import time\n",
    "\n",
    "# 응답 시간 측정\n",
    "response_times = []\n",
    "for i in range(5):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        prediction = loaded_model.predict(test_input_1)\n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        response_times.append(response_time)\n",
    "        print(f\"테스트 {i+1}: {response_time:.3f}초\")\n",
    "    except Exception as e:\n",
    "        print(f\"테스트 {i+1} 실패: {str(e)}\")\n",
    "\n",
    "if response_times:\n",
    "    avg_response_time = np.mean(response_times)\n",
    "    print(f\"평균 응답 시간: {avg_response_time:.3f}초\")\n",
    "    print(f\"최소 응답 시간: {min(response_times):.3f}초\")\n",
    "    print(f\"최대 응답 시간: {max(response_times):.3f}초\")\n",
    "\n",
    "print(\"\\n=== 실시간 추론 서비스 배포 및 테스트 완료 ===\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2-5. v2-realtime-inference-pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
